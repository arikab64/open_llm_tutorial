{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect GPU Info\n",
    "\n",
    "In Mac run ```system_profiler SPDisplaysDataType```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphics/Displays:\n",
      "\n",
      "    Apple M1:\n",
      "\n",
      "      Chipset Model: Apple M1\n",
      "      Type: GPU\n",
      "      Bus: Built-In\n",
      "      Total Number of Cores: 8\n",
      "      Vendor: Apple (0x106b)\n",
      "      Metal Support: Metal 3\n",
      "      Displays:\n",
      "        Color LCD:\n",
      "          Display Type: Built-In Retina LCD\n",
      "          Resolution: 2560 x 1600 Retina\n",
      "          Main Display: Yes\n",
      "          Mirror: Off\n",
      "          Online: Yes\n",
      "          Automatically Adjust Brightness: No\n",
      "          Connection Type: Internal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "system_profiler SPDisplaysDataType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For Mac M1 run ```install_mac_m1_prerequisites.sh```*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install llama.cpp with Hardware acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip uninstall llama-cpp-python -y\n",
    "\n",
    "CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" \\\n",
    "FORCE_CMAKE=1 \\\n",
    "pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose\n",
    "\n",
    "# OpenAI Server\n",
    "pip install llama-cpp-python[server]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download From Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_name_or_path = 'TheBloke/Llama-2-7b-Chat-GGUF'\n",
    "model_basename = 'llama-2-7b-chat.Q4_0.gguf' \n",
    "\n",
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename, local_dir='models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running OpenAI Server (LLama-2-13B Chat)\n",
    "\n",
    "```shell\n",
    "python3 -m llama_cpp.server --model models/llama-2-7b-chat.ggmlv3.q4_0.bin --n_gpu_layers 8\n",
    "```\n",
    "\n",
    "To view OpenAI docs: http://localhost:8000/docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Langchain chat\n",
    "\n",
    "Run Examples/chat.py script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
